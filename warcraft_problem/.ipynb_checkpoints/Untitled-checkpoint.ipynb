{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2573ae1-3422-4cbe-b2e8-ab235c91f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "from projectnet import ProjectNet\n",
    "from train import train_projectnet, train_with_pnet, train_net, train_with_blackbox\n",
    "import process_data \n",
    "\n",
    "from models import get_model\n",
    "from constants import *\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd11b93b-88d6-4b9e-b5de-2686a77fd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, costs_edge, weights, labels, rounds=5): \n",
    "    model.eval()\n",
    "    my_paths = get_sol(model(costs_edge, rounds=rounds), edge, map_size).to(device)\n",
    "    # print(my_paths.shape)\n",
    "    pred = torch.sum(torch.tensor(weights).flatten(start_dim = 1).to(device) * my_paths.flatten(start_dim=1), dim = 1)\n",
    "    opt = torch.sum(torch.tensor(weights).flatten(start_dim = 1).to(device) * labels, dim = 1)\n",
    "\n",
    "    return torch.mean((pred-opt)/opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21a5d23-c9fd-4528-bd48-3f20fd66fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_size = 12\n",
    "train_images, test_images, train_costs, test_costs, train_weights, test_weights, train_labels, test_labels, max_training = process_data.get_data(map_size)\n",
    "A, b, train_costs_edge, test_costs_edge, used_indices, var_cnt, edge = process_data.get_constraints(map_size, train_costs, test_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b3a492-fb92-4df8-ae52-ab72e34570b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = train_weights.shape[1] * train_weights.shape[1]\n",
    "in_channels = 3 # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc8cfdf-4cdd-4e80-b6b7-246655a8c276",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108921/2508088547.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_with_blackbox(blackbox_convnet, train_images, test_images, torch.tensor(train_weights).flatten(start_dim=1).to(device), torch.tensor(test_weights).flatten(start_dim=1).to(device), epochs = 300)\n",
      "/home/jovyan/Rares/projectnet/train.py:189: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ccc1 = torch.mean(torch.sum(torch.tensor(test_outputs).flatten(start_dim = 1) * optimal_paths_true, dim = 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "test:  5.154487609863281\n",
      "time: 41.31979823112488\n",
      "\n",
      "epoch  0  mean loss:  5.38924569606781  median  5.351196527481079\n",
      "epoch:  1\n",
      "test:  4.891226768493652\n",
      "time: 40.718390703201294\n",
      "\n",
      "epoch  1  mean loss:  4.659301364421845  median  4.63531494140625\n",
      "epoch:  2\n",
      "test:  4.798781871795654\n",
      "time: 40.336241245269775\n",
      "\n",
      "epoch  2  mean loss:  4.180210781097412  median  4.1484010219573975\n",
      "epoch:  3\n",
      "test:  4.771785736083984\n",
      "time: 40.23875617980957\n",
      "\n",
      "epoch  3  mean loss:  3.926201968193054  median  3.916369676589966\n",
      "epoch:  4\n",
      "test:  4.773237228393555\n",
      "time: 39.81436061859131\n",
      "\n",
      "epoch  4  mean loss:  3.7641948866844177  median  3.7583619356155396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m blackbox_convnet \u001b[38;5;241m=\u001b[39m get_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet18\u001b[39m\u001b[38;5;124m\"\u001b[39m, out_size, in_channels, {})\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_with_blackbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblackbox_convnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Rares/projectnet/train.py:181\u001b[0m, in \u001b[0;36mtrain_with_blackbox\u001b[0;34m(model, train_inputs, test_inputs, train_outputs, test_outputs, map_size, batch_size, epochs, lr, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m#             loss = hamming_loss(suggested_shortest_paths, out.reshape(-1, 12,12)) # Use e.g. Hamming distance as the loss function\u001b[39;00m\n\u001b[1;32m    177\u001b[0m             \n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#             print()\u001b[39;00m\n\u001b[1;32m    179\u001b[0m             loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msum(out \u001b[38;5;241m*\u001b[39m suggested_shortest_paths\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 181\u001b[0m             \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# The backward pass is handled automatically\u001b[39;00m\n\u001b[1;32m    182\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()       \n\u001b[1;32m    183\u001b[0m             mses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/my-conda-envs/rares_env/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/rares_env/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/rares_env/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "blackbox_convnet = get_model(\"ResNet18\", out_size, in_channels, {}).to(device)\n",
    "train_with_blackbox(blackbox_convnet, train_images, test_images, torch.tensor(train_weights).flatten(start_dim=1).to(device), torch.tensor(test_weights).flatten(start_dim=1).to(device), epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48aa1b1a-d24a-4407-bf87-d3324d08954d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  batch  0  mean loss:  4.4460530281066895  median  4.4460530281066895 cur:  4.4460530281066895\n",
      "epoch  0  batch  1000  mean loss:  4.014522189185733  median  4.032257556915283 cur:  4.0187225341796875\n",
      "epoch  0  batch  2000  mean loss:  3.9573524172713115  median  3.9777188301086426 cur:  3.8165829181671143\n",
      "epoch  0  batch  3000  mean loss:  3.9271531925826775  median  3.9217190742492676 cur:  3.958155393600464\n",
      "epoch  0  batch  4000  mean loss:  3.901296212349409  median  3.904123544692993 cur:  3.7527477741241455\n",
      "epoch  0  batch  5000  mean loss:  3.88099583864212  median  3.8981575965881348 cur:  3.5937066078186035\n",
      "epoch  0  batch  6000  mean loss:  3.842191617488861  median  3.8786261081695557 cur:  3.7035911083221436\n",
      "epoch  0  batch  7000  mean loss:  3.81742773771286  median  3.871638774871826 cur:  3.958035707473755\n",
      "epoch  0  batch  8000  mean loss:  3.763965985774994  median  3.830655336380005 cur:  3.7150096893310547\n",
      "epoch  0  batch  9000  mean loss:  3.7433997869491575  median  3.8023571968078613 cur:  3.9784417152404785\n",
      "epoch  1  batch  0  mean loss:  3.719537103176117  median  3.793436288833618 cur:  4.045693397521973\n",
      "epoch  1  batch  1000  mean loss:  3.6851679706573486  median  3.7827510833740234 cur:  3.7314891815185547\n",
      "epoch  1  batch  2000  mean loss:  3.65796923160553  median  3.7670211791992188 cur:  3.589750051498413\n",
      "epoch  1  batch  3000  mean loss:  3.6667857336997987  median  3.763914108276367 cur:  3.771773099899292\n",
      "epoch  1  batch  4000  mean loss:  3.648424253463745  median  3.748305082321167 cur:  3.6160149574279785\n",
      "epoch  1  batch  5000  mean loss:  3.642509639263153  median  3.743403196334839 cur:  3.5400993824005127\n",
      "epoch  1  batch  6000  mean loss:  3.6491047143936157  median  3.739763021469116 cur:  3.5613889694213867\n",
      "epoch  1  batch  7000  mean loss:  3.651648180484772  median  3.7366063594818115 cur:  3.8219354152679443\n",
      "epoch  1  batch  8000  mean loss:  3.623486933708191  median  3.7243199348449707 cur:  3.5909841060638428\n",
      "epoch  1  batch  9000  mean loss:  3.6249977231025694  median  3.7218401432037354 cur:  3.7658631801605225\n",
      "epoch  2  batch  0  mean loss:  3.6142559170722963  median  3.713811159133911 cur:  3.93208909034729\n",
      "epoch  2  batch  1000  mean loss:  3.5956703352928163  median  3.704315662384033 cur:  3.6775693893432617\n",
      "epoch  2  batch  2000  mean loss:  3.5827408409118653  median  3.7028067111968994 cur:  3.554781913757324\n",
      "epoch  2  batch  3000  mean loss:  3.6010005354881285  median  3.6983754634857178 cur:  3.7420997619628906\n",
      "epoch  2  batch  4000  mean loss:  3.586690468788147  median  3.691176414489746 cur:  3.598750591278076\n",
      "epoch  2  batch  5000  mean loss:  3.586615808010101  median  3.688162088394165 cur:  3.5223541259765625\n",
      "epoch  2  batch  6000  mean loss:  3.593191342353821  median  3.6865665912628174 cur:  3.553262948989868\n",
      "epoch  2  batch  7000  mean loss:  3.598147242069244  median  3.6848442554473877 cur:  3.7606418132781982\n",
      "epoch  2  batch  8000  mean loss:  3.5748778796195984  median  3.673093795776367 cur:  3.5778918266296387\n",
      "epoch  2  batch  9000  mean loss:  3.5801284074783326  median  3.6710710525512695 cur:  3.7002601623535156\n",
      "epoch  3  batch  0  mean loss:  3.5763115072250367  median  3.6680140495300293 cur:  3.914536714553833\n",
      "epoch  3  batch  1000  mean loss:  3.56377322435379  median  3.6614503860473633 cur:  3.6209349632263184\n",
      "epoch  3  batch  2000  mean loss:  3.5526520895957945  median  3.6604185104370117 cur:  3.5459070205688477\n",
      "epoch  3  batch  3000  mean loss:  3.5704981541633605  median  3.6550872325897217 cur:  3.696514129638672\n",
      "epoch  3  batch  4000  mean loss:  3.563701100349426  median  3.652987241744995 cur:  3.593332529067993\n",
      "epoch  3  batch  5000  mean loss:  3.5645563769340516  median  3.6515514850616455 cur:  3.5247557163238525\n",
      "epoch  3  batch  6000  mean loss:  3.572827088832855  median  3.648252487182617 cur:  3.561760902404785\n",
      "epoch  3  batch  7000  mean loss:  3.5791652631759643  median  3.646538496017456 cur:  3.7769858837127686\n",
      "epoch  3  batch  8000  mean loss:  3.558403744697571  median  3.64237642288208 cur:  3.5801897048950195\n",
      "epoch  3  batch  9000  mean loss:  3.5634004187583925  median  3.640903949737549 cur:  3.705458641052246\n",
      "epoch  4  batch  0  mean loss:  3.5584744882583617  median  3.6395351886749268 cur:  3.8645763397216797\n",
      "epoch  4  batch  1000  mean loss:  3.5455472087860107  median  3.6362736225128174 cur:  3.5742976665496826\n",
      "epoch  4  batch  2000  mean loss:  3.532148139476776  median  3.6343328952789307 cur:  3.5026040077209473\n",
      "epoch  4  batch  3000  mean loss:  3.5467893838882447  median  3.62984037399292 cur:  3.6941752433776855\n",
      "epoch  4  batch  4000  mean loss:  3.540457351207733  median  3.627624988555908 cur:  3.557124614715576\n",
      "epoch  4  batch  5000  mean loss:  3.544559512138367  median  3.627624988555908 cur:  3.523818016052246\n",
      "epoch  4  batch  6000  mean loss:  3.5553239130973817  median  3.6266136169433594 cur:  3.530402660369873\n",
      "epoch  4  batch  7000  mean loss:  3.568703098297119  median  3.6262614727020264 cur:  3.789311408996582\n",
      "epoch  4  batch  8000  mean loss:  3.5518461203575136  median  3.62432599067688 cur:  3.6156904697418213\n",
      "epoch  4  batch  9000  mean loss:  3.556043574810028  median  3.6229798793792725 cur:  3.712900400161743\n",
      "epoch  5  batch  0  mean loss:  3.5483401942253114  median  3.621403217315674 cur:  3.8187320232391357\n",
      "epoch  5  batch  1000  mean loss:  3.5384908938407897  median  3.619555950164795 cur:  3.575442314147949\n",
      "epoch  5  batch  2000  mean loss:  3.5235961079597473  median  3.619277238845825 cur:  3.508697509765625\n",
      "epoch  5  batch  3000  mean loss:  3.5377238750457765  median  3.6156904697418213 cur:  3.6891911029815674\n",
      "epoch  5  batch  4000  mean loss:  3.5294587922096254  median  3.613460063934326 cur:  3.557508707046509\n",
      "epoch  5  batch  5000  mean loss:  3.536908326148987  median  3.613460063934326 cur:  3.527778148651123\n",
      "epoch  5  batch  6000  mean loss:  3.545343930721283  median  3.613460063934326 cur:  3.5388739109039307\n",
      "epoch  5  batch  7000  mean loss:  3.5591109204292297  median  3.613344430923462 cur:  3.828874349594116\n",
      "epoch  5  batch  8000  mean loss:  3.5391520428657532  median  3.608754873275757 cur:  3.5122549533843994\n",
      "epoch  5  batch  9000  mean loss:  3.5455762696266175  median  3.608658790588379 cur:  3.67897629737854\n",
      "epoch  6  batch  0  mean loss:  3.5349379420280456  median  3.6051652431488037 cur:  3.7750065326690674\n",
      "epoch  6  batch  1000  mean loss:  3.523561623096466  median  3.6027097702026367 cur:  3.581726551055908\n",
      "epoch  6  batch  2000  mean loss:  3.5091479301452635  median  3.6018309593200684 cur:  3.42192006111145\n",
      "epoch  6  batch  3000  mean loss:  3.527354860305786  median  3.600510597229004 cur:  3.6219303607940674\n",
      "epoch  6  batch  4000  mean loss:  3.5189972591400145  median  3.5997085571289062 cur:  3.5228898525238037\n",
      "epoch  6  batch  5000  mean loss:  3.5278552079200747  median  3.600510597229004 cur:  3.462797164916992\n",
      "epoch  6  batch  6000  mean loss:  3.538172764778137  median  3.599565267562866 cur:  3.5442750453948975\n",
      "epoch  6  batch  7000  mean loss:  3.5480647492408752  median  3.598750591278076 cur:  3.853163957595825\n",
      "epoch  6  batch  8000  mean loss:  3.527246775627136  median  3.596043109893799 cur:  3.481523036956787\n",
      "epoch  6  batch  9000  mean loss:  3.534255073070526  median  3.5933806896209717 cur:  3.696993827819824\n",
      "epoch  7  batch  0  mean loss:  3.525051872730255  median  3.590365409851074 cur:  3.7971692085266113\n",
      "epoch  7  batch  1000  mean loss:  3.5142264676094057  median  3.589750051498413 cur:  3.5850768089294434\n",
      "epoch  7  batch  2000  mean loss:  3.504162302017212  median  3.588348865509033 cur:  3.4314064979553223\n",
      "epoch  7  batch  3000  mean loss:  3.5265312504768374  median  3.5865838527679443 cur:  3.5435917377471924\n",
      "epoch  7  batch  4000  mean loss:  3.516951780319214  median  3.585526943206787 cur:  3.527716875076294\n",
      "epoch  7  batch  5000  mean loss:  3.522937650680542  median  3.586501359939575 cur:  3.433213233947754\n",
      "epoch  7  batch  6000  mean loss:  3.5330116844177244  median  3.585526943206787 cur:  3.5166943073272705\n",
      "epoch  7  batch  7000  mean loss:  3.541091957092285  median  3.5853524208068848 cur:  3.801116943359375\n",
      "epoch  7  batch  8000  mean loss:  3.519150269031525  median  3.583144187927246 cur:  3.534926414489746\n",
      "epoch  7  batch  9000  mean loss:  3.528396451473236  median  3.582137107849121 cur:  3.744126796722412\n",
      "epoch  8  batch  0  mean loss:  3.520786237716675  median  3.5819294452667236 cur:  3.8129594326019287\n",
      "epoch  8  batch  1000  mean loss:  3.5096694445610046  median  3.581425905227661 cur:  3.610036611557007\n",
      "epoch  8  batch  2000  mean loss:  3.499861719608307  median  3.5812828540802 cur:  3.439581871032715\n",
      "epoch  8  batch  3000  mean loss:  3.5186195302009584  median  3.5790977478027344 cur:  3.560692071914673\n",
      "epoch  8  batch  4000  mean loss:  3.5093981790542603  median  3.5788707733154297 cur:  3.500614881515503\n",
      "epoch  8  batch  5000  mean loss:  3.5137170219421385  median  3.5790977478027344 cur:  3.405103921890259\n",
      "epoch  8  batch  6000  mean loss:  3.5228689670562745  median  3.578742265701294 cur:  3.493117094039917\n",
      "epoch  8  batch  7000  mean loss:  3.5340341973304747  median  3.5782389640808105 cur:  3.802321195602417\n",
      "epoch  8  batch  8000  mean loss:  3.5118443059921263  median  3.5747478008270264 cur:  3.5136122703552246\n",
      "epoch  8  batch  9000  mean loss:  3.519307816028595  median  3.57421875 cur:  3.7514970302581787\n",
      "epoch  9  batch  0  mean loss:  3.5151771759986876  median  3.573557138442993 cur:  3.8270440101623535\n",
      "epoch  9  batch  1000  mean loss:  3.5054250049591062  median  3.572890520095825 cur:  3.6231279373168945\n",
      "epoch  9  batch  2000  mean loss:  3.4958441853523254  median  3.572665214538574 cur:  3.450730562210083\n",
      "epoch  9  batch  3000  mean loss:  3.5195899176597596  median  3.571394681930542 cur:  3.592958688735962\n",
      "epoch  9  batch  4000  mean loss:  3.5084341859817503  median  3.570427894592285 cur:  3.5353610515594482\n",
      "epoch  9  batch  5000  mean loss:  3.512185685634613  median  3.5705840587615967 cur:  3.454167604446411\n",
      "epoch  9  batch  6000  mean loss:  3.520815932750702  median  3.5698814392089844 cur:  3.5315892696380615\n",
      "epoch  9  batch  7000  mean loss:  3.5309115290641784  median  3.56866455078125 cur:  3.710923433303833\n",
      "epoch  9  batch  8000  mean loss:  3.5052686500549317  median  3.5664961338043213 cur:  3.5240719318389893\n",
      "epoch  9  batch  9000  mean loss:  3.515331265926361  median  3.5660295486450195 cur:  3.756493330001831\n",
      "MEAN LOSS PROJECT NET:  3.507714281082153\n"
     ]
    }
   ],
   "source": [
    "from projectnet import ProjectNet\n",
    "rounds = 5\n",
    "\n",
    "projectnet = ProjectNet(A, b, var_cnt, rounds = rounds, step = 0.1, projection_tolerance = 0.01, factor = 1).to(device)\n",
    "train_projectnet(projectnet, train_costs_edge,\n",
    "                 epochs = 10, rounds = rounds, lr = 1e-5, verbose = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dcfe2c1-7a21-4313-8fb3-9c3072ada31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2325136363506317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108921/4251089180.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = torch.sum(torch.tensor(weights).flatten(start_dim = 1).to(device) * my_paths.flatten(start_dim=1), dim = 1)\n",
      "/tmp/ipykernel_108921/4251089180.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  opt = torch.sum(torch.tensor(weights).flatten(start_dim = 1).to(device) * labels, dim = 1)\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(projectnet, test_costs_edge[:100,:], test_weights[:100,:], test_labels[:100,:], rounds=5)\n",
    "print(\"test loss:\", test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633b843b-e3ff-4b90-9190-07d39086efee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with projectnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108921/3864284703.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  convnet, losses, train_losses, test_losses = train_with_pnet(convnet, projectnet, train_images, test_images, torch.tensor(train_weights).flatten(start_dim=1).to(device), torch.tensor(test_weights).flatten(start_dim=1).to(device), used_indices, edge, rounds=rounds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 batch: 0  mean loss:  48.49897003173828  median  48.49897003173828\n",
      "epoch  0 batch: 10  mean loss:  9.698039011521773  median  6.288942813873291\n",
      "epoch  0 batch: 20  mean loss:  6.614255723499117  median  4.236214637756348\n",
      "epoch  0 batch: 30  mean loss:  5.3066115840788814  median  3.4388880729675293\n",
      "epoch  0 batch: 40  mean loss:  4.649543093471992  median  2.820957899093628\n",
      "epoch  0 batch: 50  mean loss:  4.291736233468149  median  2.820957899093628\n",
      "epoch  0 batch: 60  mean loss:  4.063619945870071  median  2.851912260055542\n",
      "epoch  0 batch: 70  mean loss:  3.9183564219676272  median  2.8568029403686523\n",
      "epoch  0 batch: 80  mean loss:  3.774268173877104  median  2.851912260055542\n",
      "epoch  0 batch: 90  mean loss:  3.7155372546269345  median  2.8776681423187256\n",
      "epoch:  0\n",
      "time: 30.440244913101196\n",
      "test:  5.552981853485107\n",
      "\n",
      "epoch  1 batch: 0  mean loss:  3.2010697507858277  median  2.9190176725387573\n",
      "epoch  1 batch: 10  mean loss:  2.919193112850189  median  2.891801953315735\n",
      "epoch  1 batch: 20  mean loss:  2.881920955181122  median  2.8844497203826904\n",
      "epoch  1 batch: 30  mean loss:  2.9078103280067444  median  2.899678587913513\n",
      "epoch  1 batch: 40  mean loss:  2.9320226383209227  median  2.9295406341552734\n",
      "epoch  1 batch: 50  mean loss:  2.938465895652771  median  2.9386247396469116\n",
      "epoch  1 batch: 60  mean loss:  2.931590690612793  median  2.931578516960144\n",
      "epoch  1 batch: 70  mean loss:  2.9170987105369566  median  2.9269673824310303\n",
      "epoch  1 batch: 80  mean loss:  2.9199289178848264  median  2.926117181777954\n",
      "epoch  1 batch: 90  mean loss:  2.887747721672058  median  2.8958250284194946\n",
      "epoch:  1\n",
      "time: 32.13639843463898\n",
      "test:  5.463418483734131\n",
      "\n",
      "epoch  2 batch: 0  mean loss:  2.8705222249031066  median  2.867142677307129\n",
      "epoch  2 batch: 10  mean loss:  2.8526262927055357  median  2.8604626655578613\n",
      "epoch  2 batch: 20  mean loss:  2.8551382184028626  median  2.8604626655578613\n",
      "epoch  2 batch: 30  mean loss:  2.8547740483284  median  2.8604626655578613\n",
      "epoch  2 batch: 40  mean loss:  2.8555764150619507  median  2.8629519939422607\n",
      "epoch  2 batch: 50  mean loss:  2.8540485882759095  median  2.861098289489746\n",
      "epoch  2 batch: 60  mean loss:  2.8582576847076417  median  2.863120436668396\n",
      "epoch  2 batch: 70  mean loss:  2.8619408297538755  median  2.8753952980041504\n",
      "epoch  2 batch: 80  mean loss:  2.862453324794769  median  2.871187925338745\n",
      "epoch  2 batch: 90  mean loss:  2.8579746866226197  median  2.8750479221343994\n",
      "epoch:  2\n",
      "time: 33.69307549794515\n",
      "test:  5.26249361038208\n",
      "\n",
      "epoch  3 batch: 0  mean loss:  2.861795573234558  median  2.871187925338745\n",
      "epoch  3 batch: 10  mean loss:  2.862379643917084  median  2.8643728494644165\n",
      "epoch  3 batch: 20  mean loss:  2.8669463872909544  median  2.8750479221343994\n",
      "epoch  3 batch: 30  mean loss:  2.884363977909088  median  2.8928273916244507\n",
      "epoch  3 batch: 40  mean loss:  2.891600351333618  median  2.8928273916244507\n",
      "epoch  3 batch: 50  mean loss:  2.9625784802436828  median  2.9052082300186157\n",
      "epoch  3 batch: 60  mean loss:  3.0855948328971863  median  2.9270119667053223\n",
      "epoch  3 batch: 70  mean loss:  3.2448423171043395  median  2.9711755514144897\n",
      "epoch  3 batch: 80  mean loss:  3.4235846972465516  median  3.0331063270568848\n",
      "epoch  3 batch: 90  mean loss:  3.6188160943984986  median  3.189420700073242\n",
      "epoch:  3\n",
      "time: 38.966595232486725\n",
      "test:  4.559410095214844\n",
      "\n",
      "epoch  4 batch: 0  mean loss:  3.7969990181922912  median  3.967846989631653\n",
      "epoch  4 batch: 10  mean loss:  3.968548016548157  median  4.257577419281006\n",
      "epoch  4 batch: 20  mean loss:  4.135669810771942  median  4.400345087051392\n",
      "epoch  4 batch: 30  mean loss:  4.288475842475891  median  4.479988098144531\n",
      "epoch  4 batch: 40  mean loss:  4.444828538894654  median  4.512941360473633\n",
      "epoch  4 batch: 50  mean loss:  4.533933868408203  median  4.5238471031188965\n",
      "epoch  4 batch: 60  mean loss:  4.5649046492576595  median  4.529660701751709\n",
      "epoch  4 batch: 70  mean loss:  4.562334105968476  median  4.537576913833618\n",
      "epoch  4 batch: 80  mean loss:  4.522180998325348  median  4.514550685882568\n",
      "epoch  4 batch: 90  mean loss:  4.469959690570831  median  4.47498345375061\n",
      "epoch:  4\n",
      "time: 58.07373886108398\n",
      "test:  4.134382247924805\n",
      "\n",
      "epoch  5 batch: 0  mean loss:  4.429955952167511  median  4.434466361999512\n",
      "epoch  5 batch: 10  mean loss:  4.392320854663849  median  4.376798152923584\n",
      "epoch  5 batch: 20  mean loss:  4.3457783555984495  median  4.337199449539185\n",
      "epoch  5 batch: 30  mean loss:  4.3031670546531675  median  4.287198305130005\n",
      "epoch  5 batch: 40  mean loss:  4.260861389636993  median  4.247961521148682\n",
      "epoch  5 batch: 50  mean loss:  4.225355715751648  median  4.224584341049194\n",
      "epoch  5 batch: 60  mean loss:  4.187381188869477  median  4.176998853683472\n",
      "epoch  5 batch: 70  mean loss:  4.146284146308899  median  4.142955541610718\n",
      "epoch  5 batch: 80  mean loss:  4.115180351734161  median  4.114448070526123\n",
      "epoch  5 batch: 90  mean loss:  4.086070699691772  median  4.071361303329468\n",
      "epoch:  5\n",
      "time: 74.99054010709126\n",
      "test:  3.8438713550567627\n",
      "\n",
      "epoch  6 batch: 0  mean loss:  4.060492753982544  median  4.0424134731292725\n",
      "epoch  6 batch: 10  mean loss:  4.04260409116745  median  4.027021884918213\n",
      "epoch  6 batch: 20  mean loss:  4.029007573127746  median  4.022929668426514\n",
      "epoch  6 batch: 30  mean loss:  4.016863694190979  median  4.0190441608428955\n",
      "epoch  6 batch: 40  mean loss:  4.006607298851013  median  4.0190441608428955\n",
      "epoch  6 batch: 50  mean loss:  4.000280790328979  median  4.0125627517700195\n",
      "epoch  6 batch: 60  mean loss:  3.998847143650055  median  4.0125627517700195\n",
      "epoch  6 batch: 70  mean loss:  3.99761061668396  median  3.9951586723327637\n",
      "epoch  6 batch: 80  mean loss:  3.994241988658905  median  3.991366744041443\n",
      "epoch  6 batch: 90  mean loss:  3.9905002784729002  median  3.9872735738754272\n",
      "epoch:  6\n",
      "time: 92.67180660792759\n",
      "test:  3.7912790775299072\n",
      "\n",
      "epoch  7 batch: 0  mean loss:  3.983469135761261  median  3.9827141761779785\n",
      "epoch  7 batch: 10  mean loss:  3.9765696930885315  median  3.9595402479171753\n",
      "epoch  7 batch: 20  mean loss:  3.965655035972595  median  3.9565882682800293\n",
      "epoch  7 batch: 30  mean loss:  3.9524596762657165  median  3.940991520881653\n",
      "epoch  7 batch: 40  mean loss:  3.941160306930542  median  3.940991520881653\n",
      "epoch  7 batch: 50  mean loss:  3.926067924499512  median  3.9279394149780273\n",
      "epoch  7 batch: 60  mean loss:  3.9135378623008727  median  3.909152030944824\n",
      "epoch  7 batch: 70  mean loss:  3.9030537676811217  median  3.909152030944824\n",
      "epoch  7 batch: 80  mean loss:  3.8936396837234497  median  3.900997519493103\n",
      "epoch  7 batch: 90  mean loss:  3.8859699821472167  median  3.8805376291275024\n",
      "epoch:  7\n",
      "time: 107.2856175005436\n",
      "test:  3.7290050983428955\n",
      "\n",
      "epoch  8 batch: 0  mean loss:  3.877310814857483  median  3.8761143684387207\n",
      "epoch  8 batch: 10  mean loss:  3.8649309611320497  median  3.8620412349700928\n",
      "epoch  8 batch: 20  mean loss:  3.8613251304626464  median  3.8584734201431274\n",
      "epoch  8 batch: 30  mean loss:  3.8563504028320312  median  3.8519808053970337\n",
      "epoch  8 batch: 40  mean loss:  3.8482806944847106  median  3.8519808053970337\n",
      "epoch  8 batch: 50  mean loss:  3.849338688850403  median  3.8519808053970337\n",
      "epoch  8 batch: 60  mean loss:  3.845160720348358  median  3.846651554107666\n",
      "epoch  8 batch: 70  mean loss:  3.8398387598991395  median  3.846651554107666\n",
      "epoch  8 batch: 80  mean loss:  3.8318198847770693  median  3.843876600265503\n",
      "epoch  8 batch: 90  mean loss:  3.823034362792969  median  3.8344210386276245\n",
      "epoch:  8\n",
      "time: 119.07109843360053\n",
      "test:  3.693847417831421\n",
      "\n",
      "epoch  9 batch: 0  mean loss:  3.8140503478050234  median  3.827755928039551\n",
      "epoch  9 batch: 10  mean loss:  3.8090859484672546  median  3.822419047355652\n",
      "epoch  9 batch: 20  mean loss:  3.8033063507080076  median  3.813276529312134\n",
      "epoch  9 batch: 30  mean loss:  3.794385087490082  median  3.8026245832443237\n",
      "epoch  9 batch: 40  mean loss:  3.790156946182251  median  3.8026245832443237\n",
      "epoch  9 batch: 50  mean loss:  3.777712497711182  median  3.7804723978042603\n",
      "epoch  9 batch: 60  mean loss:  3.766428725719452  median  3.7603660821914673\n",
      "epoch  9 batch: 70  mean loss:  3.75861403465271  median  3.7603660821914673\n",
      "epoch  9 batch: 80  mean loss:  3.7535585975646972  median  3.7513400316238403\n",
      "epoch  9 batch: 90  mean loss:  3.7447453999519347  median  3.741125226020813\n",
      "epoch:  9\n",
      "time: 127.50432524681091\n",
      "test:  3.62308406829834\n",
      "\n",
      "MEAN MSE:  3.70812295293808\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_losse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain with projectnet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 8\u001b[0m convnet, losses, train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_pnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojectnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Rares/projectnet/train.py:147\u001b[0m, in \u001b[0;36mtrain_with_pnet\u001b[0;34m(model, projectnet, train_inputs, test_inputs, train_outputs, test_outputs, used_indices, edge, rounds, map_size, batch_size, epochs, lr, verbose)\u001b[0m\n\u001b[1;32m    144\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest-model-parameters.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMEAN MSE: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(mses))\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, mses, train_losses, \u001b[43mtest_losse\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_losse' is not defined"
     ]
    }
   ],
   "source": [
    "convnet = get_model(\"ResNet18\", out_size, in_channels, {}).to(device)\n",
    "convnet.load_state_dict(torch.load(\"basic-model-parameters.pt\"))\n",
    "# print(\"train 2-stage\")\n",
    "# convnet, mses, train_losses, test_losses = train_net(convnet, train_images.to(device), train_costs.to(device), batch_size = 20, epochs = 2, verbose = True)\n",
    "\n",
    "print(\"train with projectnet\")\n",
    "rounds=5\n",
    "convnet, losses, train_losses, test_losses = train_with_pnet(convnet, projectnet, train_images, test_images, torch.tensor(train_weights).flatten(start_dim=1).to(device), torch.tensor(test_weights).flatten(start_dim=1).to(device), used_indices, edge, rounds=rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e950b2-63af-4fbd-8e0f-c07c8c0e75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_images_batch = test_images[:100,:]\n",
    "test_labels_batch = test_labels[:100,:]\n",
    "test_weights_batch = test_weights[:100,:]\n",
    "pred_paths = get_paths(convnet, test_images_batch, map_size)\n",
    "test_true_dist = torch.sum(test_weights_batch.to(device).flatten(start_dim = 1) * test_labels_batch.flatten(start_dim = 1), dim = 1)\n",
    "pred_test_dist = torch.sum(test_weights_batch.to(device).flatten(start_dim = 1) * pred_paths, dim = 1)\n",
    "print(torch.sum(pred_test_dist <= test_true_dist * 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a091a0e-0507-47de-9297-c1f63761f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13036923110485077\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean((pred_test_dist - test_true_dist)/test_true_dist).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba05ec-d6b3-4e6b-8171-5fc2bed499da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rares_env",
   "language": "python",
   "name": "rares_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
